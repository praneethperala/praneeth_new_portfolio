library(rpart.plot)
library(ggplot2)
library(kableExtra)
library(caret)
# Set seed for reproducibility
set.seed(123)
# Function to generate complex data
generate_complex_data <- function(n) {
mu <- rep(0, 5)
Sigma <- matrix(0.7, nrow = 5, ncol = 5)
diag(Sigma) <- 1
data <- mvrnorm(n, mu, Sigma)
data <- as.data.frame(data)
names(data) <- c("X1", "X2", "X3", "X4", "Y")
data$Y <- 2*sin(data$X1) + 3*log(1 + exp(data$X2)) + rnorm(n)
return(data)
}
# Generate complex data
data <- generate_complex_data(300)
# Split the dataset into training and testing sets
train_index <- createDataPartition(data$Y, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Decision tree model
tree_model <- rpart(Y ~ ., data = train_data, method = "anova")
# Linear regression model
linear_model <- lm(Y ~ ., data = train_data)
# Visualize decision tree
rpart.plot(tree_model, main = "Decision Tree Model", extra = 101, under = TRUE, compress = TRUE)
# Visualize linear regression
ggplot(train_data, aes(x = Y, y = predict(linear_model))) +
geom_point() +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
ggtitle("Linear Regression Model")
# Display decision tree model summary using kableExtra
tree_summary <- summary(tree_model)
# Display linear regression model summary using kableExtra
linear_summary <- summary(linear_model)
# Display decision tree model summary
tree_summary <- summary(tree_model)
print("Decision Tree Model Summary:")
print(tree_summary$splits)
# Display linear regression model summary
linear_summary <- summary(linear_model)
print("\nLinear Regression Model Summary:")
print(linear_summary)
# Visualization of decision tree
plot(tree_model, uniform = TRUE, main = "Decision Tree Model")
# Visualization of linear regression
par(mfrow = c(1, 1))  # Reset the plotting layout
plot(train_data$Y, predict(linear_model), main = "Linear Regression Model",
xlab = "Actual Y", ylab = "Predicted Y")
abline(a = 0, b = 1, col = "red", lty = 2)
install.packages("randomForest")
install.packages("ggplot2")
# Load required libraries
library(randomForest)
library(ggplot2)
# Set a seed for reproducibility
set.seed(123)
# Generate a random dataset with two features (X1 and X2) and a binary target variable (Y)
n <- 100
data <- data.frame(
X1 = rnorm(n),
X2 = rnorm(n),
Y = factor(sample(0:1, n, replace = TRUE))
)
# Split the dataset into training and testing sets
train_indices <- sample(1:n, 0.8 * n)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train a random forest model
rf_model <- randomForest(Y ~ X1 + X2, data = train_data, ntree = 100)
# Make predictions on the test set
predictions <- predict(rf_model, newdata = test_data)
# Evaluate model accuracy
accuracy <- mean(predictions == test_data$Y)
cat("Model Accuracy:", accuracy, "\n")
# Visualize the random forest results
# Since we have two features, we can create a scatter plot
ggplot(data, aes(x = X1, y = X2, color = Y)) +
geom_point() +
geom_point(data = test_data, aes(x = X1, y = X2), color = "black", size = 3, alpha = 0.5) +
ggtitle("Random Forest Classification") +
theme_minimal()
# Run this code chunk without changing it
# Call the necessary packages
library(dplyr)
library(rpart)
library(ipred)
library(caret)
library(ggplot2)
library(randomForest)
library(knitr)
# read the csv file in R and name it as OFP_data
OFP_data<-read.csv("MP2_data_option2.csv", head=TRUE)
MP2_data_option2.1 <- read.csv("C:/Users/MY_PC/Downloads/Mini-3-Dec-10/MP2_data_option2-1.csv")
View(MP2_data_option2.1)
# Call the necessary packages
library(dplyr)
library(rpart)
library(ipred)
library(caret)
library(ggplot2)
library(randomForest)
library(knitr)
# read the csv file in R and name it as OFP_data
OFP_data<-read.csv("C:/Users/MY_PC/Downloads/Mini-3-Dec-10/MP2_data_option2-1.csv" , head=TRUE)
# Your code for Task 1  goes below
# Declare the specified variables as categorical (factor)
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
OFP_data[categorical_vars] <- lapply(OFP_data[categorical_vars], factor)
# Your code for Task 1  goes below
# Declare the specified variables as categorical (factor)
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
OFP_data[categorical_vars] <- lapply(OFP_data[categorical_vars], factor)
# Call the necessary packages
library(dplyr)
library(rpart)
library(ipred)
library(caret)
library(ggplot2)
library(randomForest)
library(knitr)
# read the csv file in R and name it as OFP_data
OFP_data<-read.csv("C:/Users/MY_PC/Downloads/Mini-3-Dec-10/MP2_data_option2-1.csv" , head=TRUE)
# Your code for Task 1  goes below
# Declare the specified variables as categorical (factor)
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
OFP_data[categorical_vars] <- lapply(OFP_data[categorical_vars], factor)
# Your code for Task 1  goes below
# Assuming your data frame is named OFP_data
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
#print(table(OFP_train$hlth))
# Your code for Task 1  goes below
# Assuming your data frame is named OFP_data
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
# Assuming column_names is a vector of column names
categorical_vars <- c("adldiff", "black", "sex", "maried", "employed", "privins", "medicaid", "region", "hlth")
# Use mutate_at to convert specified columns to factors
OFP_data <- OFP_data %>%
mutate_at(vars(all_of(categorical_vars)), as.factor)
#print(table(OFP_train$hlth))
# Your code for Task 1  goes below
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
# Assuming column_names is a vector of column names
categorical_vars <- c("adldiff", "black", "sex", "maried", "employed", "privins", "medicaid", "region", "hlth")
# Use mutate_at to convert specified columns to factors
OFP_data <- OFP_data %>%
mutate_at(vars(all_of(categorical_vars)), as.factor)
# Create a vector to represent the outcome variable
y <- OFP_data$hlth
# Use createDataPartition to split the data
index <- createDataPartition(y, p = 0.7, list = FALSE)
# Create training and testing sets
OFP_train <- OFP_data[index, ]
OFP_test <- OFP_data[-index, ]
#print(table(OFP_train$hlth))
# Your code for Task 1 goes below
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
# Assuming column_names is a vector of column names
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
# Use mutate_at to convert specified columns to factors
OFP_data <- OFP_data %>%
mutate_at(vars(all_of(categorical_vars)), as.factor)
# Your code for Task 1 goes below
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
# Assuming column_names is a vector of column names
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
# Use mutate_at to convert specified columns to factors
OFP_data <- OFP_data %>%
mutate_at(vars(matches(paste(categorical_vars, collapse = "|"))), as.factor)
# Create a vector to represent the outcome variable
y <- OFP_data$hlth
# Use createDataPartition to split the data
index <- createDataPartition(y, p = 0.7, list = FALSE)
# Create training and testing sets
OFP_train <- OFP_data[index, ]
OFP_test <- OFP_data[-index, ]
# Print the levels of the 'hlth' variable in the training set
print(table(OFP_train$hlth))
# Run this code chunk without changing it
# clear the session
rm(list=ls())
# Call the necessary packages
library(dplyr)
library(rpart)
library(ipred)
library(caret)
library(ggplot2)
library(randomForest)
library(knitr)
# read the csv file in R and name it as OFP_data
OFP_data<-read.csv("C:/Users/MY_PC/Downloads/Mini-3-Dec-10/MP2_data_option2-1.csv" , head=TRUE)
# Your code for Task 1 goes below
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
# Assuming column_names is a vector of column names
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
# Use mutate_at to convert specified columns to factors
OFP_data <- OFP_data %>%
mutate_at(vars(matches(paste(categorical_vars, collapse = "|"))), as.factor)
# Create a vector to represent the outcome variable
y <- OFP_data$hlth
# Use createDataPartition to split the data
index <- createDataPartition(y, p = 0.7, list = FALSE)
# Create training and testing sets
OFP_train <- OFP_data[index, ]
OFP_test <- OFP_data[-index, ]
# Print the levels of the 'hlth' variable in the training set
print(table(OFP_train$hlth))
# Enter your code for Task  2  below
# Set seed for reproducibility
set.seed(22222)
# Load the randomForest package
library(randomForest)
# Specify predictors (excluding the outcome variable 'hlth')
predictors <- setdiff(names(OFP_train), "hlth")
# Train the random forest model with default parameters
rf_default <- randomForest(hlth ~ ., data = OFP_train[, c(predictors, "hlth")])
# Print the model summary
print(rf_default)
# Enter your code for Task 3  below
# Predict hlth levels in OFP_test using rf_default model
rf_default_predict <- predict(rf_default, newdata = OFP_test, type = "response")
# Print the predicted values
print(rf_default_predict)
# Enter your code for Task 4  below
# Your code should produce the accuracy ratio
# Calculate accuracy directly
rf_default_accuracy <- sum(OFP_test$hlth == rf_default_predict) / nrow(OFP_test)
# Print the accuracy
print(paste("Random Forest Default Model Accuracy:", rf_default_accuracy))
# Enter your code for Task 5  below
# Set seed for reproducibility
set.seed(22222)
# Modify rf_default model
rf_default2 <- randomForest(
hlth ~ .,
data = OFP_train[, c(predictors, "hlth")],
ntree = 600,
mtry = 3,
nodesize = 3,
maxnodes = 300
)
# Print the modified model summary
print(rf_default2)
# Enter your code for Task 6  below
# Predict hlth levels in OFP_test using rf_default2 model
rf_default2_predict <- predict(rf_default2, newdata = OFP_test, type = "response")
# Print the predicted values
print(rf_default2_predict)
# Calculate accuracy directly for rf_default2
rf_default2_accuracy <- sum(OFP_test$hlth == rf_default2_predict) / nrow(OFP_test)
# Print the accuracy for rf_default2
print(paste("Random Forest Modified Model (rf_default2) Accuracy:", rf_default2_accuracy))
# Enter your code for Task 7  below
# Set seed for reproducibility
set.seed(22222)
# Define the training control parameters
ctrl <- trainControl(method = "cv", number = 10, seed = 22222)
# Specify the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Train the random forest model with caret
rf_default3 <- train(
hlth ~ .,
data = OFP_train[, c(predictors, "hlth")],
method = "rf",
trControl = ctrl,
tuneGrid = tuneGrid,
metric = 'Accuracy'
)
# Enter your code for Task 7  below
# Set seed for reproducibility
set.seed(22222)
# Define the training control parameters with correct seed format
ctrl <- trainControl(method = "cv", number = 10, seeds = list(seed = sample.int(10)))
# Specify the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Train the random forest model with caret
rf_default3 <- train(
hlth ~ .,
data = OFP_train[, c(predictors, "hlth")],
method = "rf",
trControl = ctrl,
tuneGrid = tuneGrid,
metric = 'Accuracy'
)
# Enter your code for Task 7  below
# Load the caret package
library(caret)
# Set seed for reproducibility
set.seed(22222)
# Define the training control parameters with correct seed format
ctrl <- trainControl(method = "cv", number = 10, seeds = list(seed = sample.int(10), final = 123))
# Specify the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Train the random forest model with caret
rf_default3 <- train(
hlth ~ .,
data = OFP_train[, c(predictors, "hlth")],
method = "rf",
trControl = ctrl,
tuneGrid = tuneGrid,
metric = 'Accuracy'
)
# Enter your code for Task 7  below
# Set seed for reproducibility
set.seed(22222)
# Specify the training control parameters
ctrl <- trainControl(method = "cv", number = 10)
# Define the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Fit the random forest model with hyperparameter tuning
rf_default3 <- train(
x = OFP_train_features,
y = OFP_train$hlth,
method = "rf",
metric = 'Accuracy',
tuneGrid = tuneGrid,
trControl = ctrl
)
# Enter your code for Task 7  below
# Set seed for reproducibility
set.seed(22222)
# Specify the training control parameters
ctrl <- trainControl(method = "cv", number = 10)
# Define the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Fit the random forest model with hyperparameter tuning
rf_default3 <- train(
x = OFP_train,
y = OFP_train$hlth,
method = "rf",
metric = 'Accuracy',
tuneGrid = tuneGrid,
trControl = ctrl
)
# Print the optimal mtry value and corresponding accuracy
print(paste("Optimal mtry:", rf_default3$bestTune$mtry))
print(paste("Cross-validated average accuracy:", max(rf_default3$results$Accuracy)))
# Enter your code for Task 7  below
# Set seed for reproducibility
set.seed(22222)
# Specify the training control parameters
ctrl <- trainControl(method = "cv", number = 10)
# Define the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Fit the random forest model with hyperparameter tuning
rf_default3 <- train(
x = OFP_train[, -which(names(OFP_train) == "hlth")],  # Exclude the target variable
y = OFP_train$hlth,
method = "rf",
metric = 'Accuracy',
tuneGrid = tuneGrid,
trControl = ctrl
)
# Enter your code for Task 7  below
# Set seed for reproducibility
set.seed(22222)
# Specify the training control parameters
ctrl <- trainControl(method = "cv", number = 10)
# Define the tuning grid for mtry
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Fit the random forest model with hyperparameter tuning
rf_default3 <- train(
x = OFP_train[, -which(names(OFP_train) == "hlth")],  # Exclude the target variable
y = OFP_train$hlth,
method = "rf",
metric = 'Accuracy',
tuneGrid = tuneGrid,
trControl = ctrl
)
# Run this code chunk without changing it
# clear the session
rm(list=ls())
# Call the necessary packages
library(dplyr)
library(rpart)
library(ipred)
library(caret)
library(ggplot2)
library(randomForest)
library(knitr)
# read the csv file in R and name it as OFP_data
OFP_data<-read.csv("C:/Users/MY_PC/Downloads/Mini-3-Dec-10/MP2_data_option2-1.csv" , head=TRUE)
# Your code for Task 1 goes below
column_names <- colnames(OFP_data)
# Print the column names
print(column_names)
# Assuming column_names is a vector of column names
categorical_vars <- c("adldiff", "black", "sex", "married", "employed", "privins", "medicaid", "region", "hlth")
# Use mutate_at to convert specified columns to factors
OFP_data <- OFP_data %>%
mutate_at(vars(matches(paste(categorical_vars, collapse = "|"))), as.factor)
# Create a vector to represent the outcome variable
y <- OFP_data$hlth
# Use createDataPartition to split the data
index <- createDataPartition(y, p = 0.7, list = FALSE)
# Create training and testing sets
OFP_train <- OFP_data[index, ]
OFP_test <- OFP_data[-index, ]
# Print the levels of the 'hlth' variable in the training set
print(table(OFP_train$hlth))
# Enter your code for Task  2  below
# Set seed for reproducibility
set.seed(22222)
# Load the randomForest package
library(randomForest)
# Specify predictors (excluding the outcome variable 'hlth')
predictors <- setdiff(names(OFP_train), "hlth")
# Train the random forest model with default parameters
rf_default <- randomForest(hlth ~ ., data = OFP_train[, c(predictors, "hlth")])
# Print the model summary
print(rf_default)
# Enter your code for Task 3  below
# Predict hlth levels in OFP_test using rf_default model
rf_default_predict <- predict(rf_default, newdata = OFP_test, type = "response")
# Print the predicted values
print(rf_default_predict)
# Enter your code for Task 4  below
# Your code should produce the accuracy ratio
# Calculate accuracy directly
rf_default_accuracy <- sum(OFP_test$hlth == rf_default_predict) / nrow(OFP_test)
# Print the accuracy
print(paste("Random Forest Default Model Accuracy:", rf_default_accuracy))
# Enter your code for Task 5  below
# Set seed for reproducibility
set.seed(22222)
# Modify rf_default model
rf_default2 <- randomForest(
hlth ~ .,
data = OFP_train[, c(predictors, "hlth")],
ntree = 600,
mtry = 3,
nodesize = 3,
maxnodes = 300
)
# Print the modified model summary
print(rf_default2)
# Enter your code for Task 6  below
# Predict hlth levels in OFP_test using rf_default2 model
rf_default2_predict <- predict(rf_default2, newdata = OFP_test, type = "response")
# Print the predicted values
print(rf_default2_predict)
# Calculate accuracy directly for rf_default2
rf_default2_accuracy <- sum(OFP_test$hlth == rf_default2_predict) / nrow(OFP_test)
# Print the accuracy for rf_default2
print(paste("Random Forest Modified Model (rf_default2) Accuracy:", rf_default2_accuracy))
# NOTE: After splitting data into OFP_train and OFP_test, you can run (just delete the # sign in front of each executable code) the following lines of codes with no error.
## get the features column in OFP_train
OFP_train_features<-OFP_train%>%
select(-hlth)
set.seed(22222)
tune_mtry <- tuneRF(
x          = OFP_train_features,
y          = OFP_train$hlth,
ntreeTry   = 50,
mtryStart  = 18,
stepFactor = 1.5,
improve    = 0.001,
plot = TRUE,
trace      = TRUE
)
tune_mtry
# Enter your code for Task 7  below
library(caret)
set.seed(22222)
# Define the tuning grid
tuneGrid <- expand.grid(mtry = seq(2, 20, by = 2))
# Train the random forest model with 10-fold cross-validation
rf_default3 <- train(
x = OFP_train_features,
y = OFP_train$hlth,
method = "rf",
metric = "Accuracy",
trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE),
tuneGrid = tuneGrid
)
# Print the results
print(rf_default3)
# Extract the optimal mtry value and corresponding accuracy
optimal_mtry <- rf_default3$bestTune$mtry
cv_accuracy <- max(rf_default3$results$Accuracy)
cat("Optimal mtry value:", optimal_mtry, "\n")
cat("Cross-validated average accuracy rate:", cv_accuracy, "\n")
# Enter your code for Task 8  below
# Predict hlth levels in OFP_test using rf_default3 model
rf_default3_predict <- predict(rf_default3, newdata = OFP_test_features)
# Enter your code for Task 8  below
# Predict health levels in OFP_test dataset
rf_default3_predict <- predict(rf_default3, newdata = OFP_test)
# Check factor levels and ensure they match between training and testing datasets
levels_train <- levels(OFP_train$hlth)
levels_test <- levels(OFP_test$hlth)
if (!identical(levels_train, levels_test)) {
stop("Factor levels of 'hlth' variable do not match between training and testing datasets.")
}
# Calculate accuracy ratio
rf_default3_accuracy <- sum(rf_default3_predict == OFP_test$hlth) / length(rf_default3_predict)
cat("Random Forest Default Model Accuracy on OFP_test:", rf_default3_accuracy, "\n")
