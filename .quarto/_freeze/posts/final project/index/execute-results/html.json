{
  "hash": "16d9c7bb5cd9f5c832dd2a8f2abaa87c",
  "result": {
    "markdown": "---\ntitle: \"Regression model using random forest\"\nauthor: \"Sai manikanta praneeth perala\"\neditor: visual\ndate: \"2024-01-06\"\ncategories: [ code, analysis, mchine learning model, R]\nimage: \"image.jpg\"\n---\n\n\n# Random Forest\n\nThe R code demonstrates the construction of a random forest classification model using a synthetic dataset with two features (X1 and X2) and a binary target variable (Y). The model is trained on 80% of the data and tested on the remaining 20%. The random forest consists of 100 trees. \\## INTRODUCTION\n\nThe model accuracy is then evaluated on the test set, yielding an accuracy of 0.5. An accuracy of 0.5 suggests that the model is performing no better than random chance in predicting the binary outcome. In this case, the model is not effectively capturing the underlying patterns in the data, and further exploration, feature engineering, or parameter tuning may be necessary to improve its performance. The scatter plot visually represents the data points and their classification, where the black points correspond to the instances in the test set. The lack of clear separation in the plot indicates the model's struggle in distinguishing between the two classes based on the given features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:randomForest':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\n# Set a seed for reproducibility\nset.seed(123)\n\n# Generate a random dataset with two features (X1 and X2) and a binary target variable (Y)\nn <- 100\ndata <- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  Y = factor(sample(0:1, n, replace = TRUE))\n)\n\n# Split the dataset into training and testing sets\ntrain_indices <- sample(1:n, 0.8 * n)\ntrain_data <- data[train_indices, ]\ntest_data <- data[-train_indices, ]\n\n# Train a random forest model\nrf_model <- randomForest(Y ~ X1 + X2, data = train_data, ntree = 100)\n\n# Make predictions on the test set\npredictions <- predict(rf_model, newdata = test_data)\n\n# Evaluate model accuracy\naccuracy <- mean(predictions == test_data$Y)\ncat(\"Model Accuracy:\", accuracy, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Accuracy: 0.5 \n```\n:::\n\n```{.r .cell-code}\n# Visualize the random forest results\n# Since we have two features, we can create a scatter plot\nggplot(data, aes(x = X1, y = X2, color = Y)) +\n  geom_point() +\n  geom_point(data = test_data, aes(x = X1, y = X2), color = \"black\", size = 3, alpha = 0.5) +\n  ggtitle(\"Random Forest Classification\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}